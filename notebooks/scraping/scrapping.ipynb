{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), \"log\"))\n",
    "\n",
    "from my_logger import setup_logging, get_logger\n",
    "setup_logging()\n",
    "log = get_logger(\"scraping\")\n",
    "log.info(\"Вызов\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QOoNBUiLjwYq",
    "outputId": "18bfb7c1-80f9-4b24-9255-c059ab02f535"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.32.5)\n",
      "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (4.14.2)\n",
      "Requirement already satisfied: lxml in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (6.0.2)\n",
      "Requirement already satisfied: selenium in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (4.38.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests) (2025.10.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: trio<1.0,>=0.31.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from selenium) (0.32.0)\n",
      "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from selenium) (1.9.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from trio<1.0,>=0.31.0->selenium) (25.4.0)\n",
      "Requirement already satisfied: sortedcontainers in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from trio<1.0,>=0.31.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from trio-websocket<1.0,>=0.12.2->selenium) (1.3.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.16.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "/bin/bash: apt-get: command not found\n",
      "/bin/bash: apt-get: command not found\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4 lxml selenium\n",
    "!apt-get update\n",
    "!apt-get install -y chromium-browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "eCysGIvBlGG-"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time # для задержек между запросами (чтобы не блокировал сайт)\n",
    "from typing import Dict, List, Optional\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "ByICJGAslITm"
   },
   "outputs": [],
   "source": [
    "class UnipageScraper:\n",
    "\n",
    "    def __init__(self, delay: float = 1.5):\n",
    "        self.base_url = \"https://www.unipage.net\"\n",
    "        self.delay = delay\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36' # чтоб не заблокали\n",
    "        })\n",
    "\n",
    "        log.info(f\"Инициализирован UnipageScraper: delay={self.delay}\")\n",
    "\n",
    "    # загрузка страницы (по умолчанию request, если сработала ошибка (мало информации или в дальнейшем не нашел карточку университета), то пробуем вытащить данные, используя библиотеку selenium)\n",
    "    def get_page(self, url: str, retries: int = 3) -> Optional[BeautifulSoup]: # хотим получить html-код страницы (пробуем 3 раза)\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                 # вначале через request\n",
    "                log.debug(f\"[GET] попытка {attempt+1}/{retries}: {url}\")\n",
    "                response = self.session.get(url, timeout=30)\n",
    "                response.raise_for_status() # если сайт выдает ошибку (например, 404), то сразу переходим к блоку - except\n",
    "\n",
    "                html = response.text # сохраняем html код страницы\n",
    "                if len(html) < 500 or \"University\" not in html: # если страница пустая или слишком мало инфы- пробуем через Selenium\n",
    "                    raise ValueError(\"Пробуем через Selenium\") # вызываем ошибку в таком случае\n",
    "\n",
    "                time.sleep(self.delay)\n",
    "                log.info(f\"[GET] ok через requests: {url}\")\n",
    "                return BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "            # если что-то не так: (сообщает об ошибке, но продолжает работать)\n",
    "            except Exception as e:\n",
    "                log.warning(f\"[GET] попытка {attempt+1} не удалась для {url}: {e}\")\n",
    "\n",
    "                if attempt == retries - 1:           # если это была последняя попытка - переходим к selenium\n",
    "                    log.info(f\"[SELENIUM] старт headless для {url}\")\n",
    "\n",
    "                    # настройка драйвера\n",
    "                    opt = Options()\n",
    "                    opt.add_argument(\"--headless=new\") # без окна\n",
    "                    opt.add_argument(\"--no-sandbox\")\n",
    "                    opt.add_argument(\"--disable-dev-shm-usage\")\n",
    "                    opt.add_argument(\"--disable-gpu\")\n",
    "                    opt.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "                    # запуск драйвера\n",
    "                    driver = webdriver.Chrome(options=opt)\n",
    "                    try:\n",
    "                        \n",
    "                        driver.get(url)\n",
    "                        time.sleep(2.5)\n",
    "                        html = driver.page_source\n",
    "                        log.info(f\"[SELENIUM] ok: {url}\")\n",
    "                        return BeautifulSoup(html, \"html.parser\")\n",
    "                    except Exception:\n",
    "                        log.exception(f\"[SELENIUM] ошибка при загрузке {url}\")\n",
    "                    finally:\n",
    "                        driver.quit()\n",
    "\n",
    "                time.sleep(5)\n",
    "                \n",
    "        log.error(f\"[GET] не удалось загрузить страницу после {retries} попыток: {url}\")\n",
    "        return None # если все попытки не удались - None\n",
    "\n",
    "    # доп ф-ия (извлечь число из текста, например, для рейтинга)\n",
    "    def extract_number(self, text: str) -> str:\n",
    "        match = re.search(r'[\\d,]+', text) # ищем посл-сть любых цифры 0-9 (\\d) и запятые\n",
    "        return match.group(0) if match else \"\"\n",
    "\n",
    "    # парсинг универа на главной странице (name, country, location (city), ranking_preview: QS, THE и ARWU, стоимость (degree type(bachelor, master и doctor)))\n",
    "    def parse_main_page_card(self, card) -> Dict:\n",
    "        data = {}\n",
    "\n",
    "        # название университета и ссылка\n",
    "        title_elem = card.select_one('.generated-card__title a') # ищем первый тег (select_one()) <a>, который находится внутри элемента с классом .generated-card__title\n",
    "        if title_elem:\n",
    "            data['name'] = title_elem.get_text(strip=True) # достаем название между тегами (без отступов)\n",
    "            data['url'] = self.base_url + title_elem.get('href', '') # достаём ссылку из атрибута тега <href> и соединяем\n",
    "        else:\n",
    "            data['name'] = \"\"\n",
    "            data['url'] = \"\"\n",
    "\n",
    "        # cтрана (по флагу)\n",
    "        flag_elem = card.select_one('.flag') # ищем все элем с классом .flag\n",
    "        if flag_elem:\n",
    "            flag_class = [c for c in flag_elem.get('class', []) if c.startswith('flag-')] # извлекаем список классов этого элем и отбираем только те, которе начинаются с 'flag-'\n",
    "            data['country'] = flag_class[0].replace('flag-', '').upper() if flag_class else \"\" # теперь для каждого в списке удаляем префикс 'flag-', получаем страну и записываем в вер[нем регистре\n",
    "        else:\n",
    "            data['country'] = \"\"\n",
    "\n",
    "        # город или локация\n",
    "        location_spans = card.select('.generated-card__row span') # ищем все элем с классом .generated-card__row span\n",
    "        if len(location_spans) >= 2:\n",
    "            data['location'] = location_spans[1].get_text(strip=True) # на странице есть несколько <span> и второй из них содержит город, поэтому мы берём location_spans[1]\n",
    "        else:\n",
    "            data['location'] = \"\"\n",
    "\n",
    "        # рейтинги QS, THE, ARWU\n",
    "        data['qs_ranking_preview'] = \"\"\n",
    "        data['the_ranking_preview'] = \"\"\n",
    "        data['arwu_ranking_preview'] = \"\"\n",
    "\n",
    "        tags = card.select('.tag_secondary') # ищем все элем с классом .tag_secondary\n",
    "        for tag in tags:\n",
    "            text = tag.get_text(strip=True)\n",
    "            # исп ф-ию, которая описана выше, чтоб вытащить цифры\n",
    "            if 'QS' in text:\n",
    "                data['qs_ranking_preview'] = self.extract_number(text)\n",
    "            elif 'THE' in text:\n",
    "                data['the_ranking_preview'] = self.extract_number(text)\n",
    "            elif 'ARWU' in text:\n",
    "                data['arwu_ranking_preview'] = self.extract_number(text)\n",
    "\n",
    "        # cтоимость обучения\n",
    "        data['bachelor_from'] = \"\"\n",
    "        data['bachelor_to'] = \"\"\n",
    "        data['master_from'] = \"\"\n",
    "        data['master_to'] = \"\"\n",
    "        data['doctorate_from'] = \"\"\n",
    "        data['doctorate_to'] = \"\"\n",
    "\n",
    "        tuition_cards = card.select('.content-card_inverted') # элем с классом .content-card_inverted, где указана стоимость\n",
    "        for tuition_card in tuition_cards: # перебираем все карточки\n",
    "            dt = tuition_card.select_one('dt') # ищем элемент <dt> - в нём написано для какой степени (бакалавр, магистр и тд)\n",
    "            if not dt:\n",
    "                continue\n",
    "\n",
    "            degree_type = dt.get_text(strip=True) # извлекаем из <dt> текст (удаляя лишние пробелы)\n",
    "            dds = tuition_card.select('dd .content-card-number__number') # ищем все элем с классом .content-card-number__number (cписок) внутри тега <dd>\n",
    "\n",
    "            if degree_type == 'Bachelor' and len(dds) >= 2:\n",
    "                data['bachelor_from'] = dds[0].get_text(strip=True)\n",
    "                data['bachelor_to'] = dds[1].get_text(strip=True)\n",
    "            elif degree_type == 'Master' and len(dds) >= 2:\n",
    "                data['master_from'] = dds[0].get_text(strip=True)\n",
    "                data['master_to'] = dds[1].get_text(strip=True)\n",
    "            elif degree_type == 'Doctorate' and len(dds) >= 2:\n",
    "                data['doctorate_from'] = dds[0].get_text(strip=True)\n",
    "                data['doctorate_to'] = dds[1].get_text(strip=True)\n",
    "                \n",
    "        if data.get('name'):\n",
    "            log.debug(f\"[CARD] {data['name']} | {data.get('country','')} | {data.get('location','')}\")\n",
    "        return data\n",
    "\n",
    "    # парсинг страницы каждого универа (location full, establishment year, students (кол-во), international students, rating - QS, THE и USA, female students, international students, acceptance rate)\n",
    "    def parse_university_page(self, url: str) -> Dict:\n",
    "        soup = self.get_page(url) # возвращаем объект BeautifulSoup (HTML-страница)\n",
    "        if not soup:\n",
    "            log.warning(f\"[PAGE] пропуск — не удалось получить HTML: {url}\")\n",
    "            return {}\n",
    "\n",
    "        data = {}\n",
    "\n",
    "        infographic_cards = soup.select('.infographic-card__main') # вся нужная нам инфа в элементах с классом .infographic-card__main\n",
    "        for card in infographic_cards: # перебор\n",
    "            dt = card.select_one('dt.infographic-card__secondary-text') # название показателя\n",
    "            dd = card.select_one('dd.infographic-card__primary-text') # его значение\n",
    "\n",
    "            if dt and dd: # еслм оба есть, то берем текст без лишних пробелов\n",
    "                key = dt.get_text(strip=True)\n",
    "                value = dd.get_text(strip=True)\n",
    "\n",
    "                if key == 'Location':\n",
    "                    data['location_full'] = value\n",
    "                elif key == 'Establishment year':\n",
    "                    data['establishment_year'] = value\n",
    "                elif key == 'Students':\n",
    "                    data['total_students'] = value\n",
    "                elif key == 'International students':\n",
    "                    data['international_students'] = value\n",
    "                elif key == 'Female students':\n",
    "                    data['female_students'] = value\n",
    "                elif key == 'Acceptance rate':\n",
    "                    data['acceptance_rate'] = value\n",
    "\n",
    "        # теперь чекаем рейтинги\n",
    "        data['qs_rating'] = \"\"\n",
    "        data['the_rating'] = \"\"\n",
    "        data['rating_usa'] = \"\"\n",
    "\n",
    "        rating_buttons = soup.select('.chart-update-button') # вся нужная нам инфа в элементах с классом .chart-update-button\n",
    "        for button in rating_buttons: # перебор\n",
    "            title_elem = button.select_one('.content-card__title') # показатель\n",
    "            number_elem = button.select_one('.content-card-number__number') # значение\n",
    "\n",
    "            if title_elem and number_elem:\n",
    "                title = title_elem.get_text(strip=True)\n",
    "                number = number_elem.get_text(strip=True)\n",
    "\n",
    "                if 'Rating QS' in title:\n",
    "                    data['qs_rating'] = number\n",
    "                elif 'Rating THE' in title:\n",
    "                    data['the_rating'] = number\n",
    "                elif 'Rating in the USA' in title:\n",
    "                    data['rating_usa'] = number\n",
    "        log.debug(f\"[PAGE] распарсено полей: {len([v for v in data.values() if v])} | {url}\")\n",
    "        return data\n",
    "\n",
    "    # cбор списков универов (перебираем страницы, пока не найдем нужное кол-во (10к+), и сохраняем в список ссылки карточек универов)\n",
    "    def scrape_universities_list(self, base_list_url: str, max_universities: int = 50) -> List[Dict]: # (по умолчанию 50, но мы потом говорим свое значение)\n",
    "        universities = []\n",
    "        page = 1\n",
    "\n",
    "        # перебор страниц, пока не соберем нужное кол-во универов\n",
    "        while len(universities) < max_universities:\n",
    "            if page == 1:\n",
    "                url = base_list_url\n",
    "\n",
    "            else:\n",
    "                url = f\"{base_list_url}?page={page}\"\n",
    "\n",
    "            log.info(f\"[LIST] страница {page}: {url}\")\n",
    "\n",
    "            soup = self.get_page(url) # через get_page() берём HTML страницы\n",
    "\n",
    "            if not soup:\n",
    "                print(f\"Не удалось загрузить страницу {page}\")\n",
    "                log.error(f\"[LIST] не удалось загрузить страницу {page}\")\n",
    "                break\n",
    "\n",
    "            cards = soup.select('.generated-card') # получаем список всех карточек на странице\n",
    "\n",
    "            if not cards:\n",
    "                print(f\"На странице {page} не найдено карточек. Завершение.\")\n",
    "                log.warning(f\"на странице {page} [LIST] карточек не найдено - Завершение.\")\n",
    "                break\n",
    "\n",
    "            print(f\"Найдено карточек на странице {page}: {len(cards)}\")\n",
    "            log.info(f\"[LIST] найдено карточек: {len(cards)} (стр. {page})\")\n",
    "\n",
    "            for card in cards:\n",
    "                if len(universities) >= max_universities:\n",
    "                    break\n",
    "\n",
    "                # теперь для каждой карточки вызываем парсер:\n",
    "                uni_data = self.parse_main_page_card(card)\n",
    "                if uni_data.get('url'):\n",
    "                    universities.append(uni_data)\n",
    "\n",
    "            print(f\"Всего собрано университетов: {len(universities)}\")\n",
    "            log.info(f\"[LIST] всего собрано университетов: {len(universities)}\")\n",
    "\n",
    "\n",
    "            if len(universities) >= max_universities:\n",
    "                break\n",
    "\n",
    "            page += 1\n",
    "\n",
    "        return universities\n",
    "\n",
    "    # полный сбор всех данных (главная ф-ия, она вызывает остальные в этом классе)\n",
    "    def scrape_all(self, list_url: str, output_file: str = 'universities.csv',\n",
    "                   max_universities: int = 50, save_every: int = 25):\n",
    "        log.info(\"[ALL] начинаем сбор ссылок на университеты…\")\n",
    "\n",
    "        # сначала вызываем scrape_universities_list - список словарей, где у каждого универа будут уже данные (которые с главной страницы)\n",
    "        universities = self.scrape_universities_list(list_url, max_universities)\n",
    "\n",
    "        log.info(f\"[ALL] к обработке: {len(universities)} университетов\")\n",
    "\n",
    "        all_data = []\n",
    "\n",
    "        # проходимся по универам\n",
    "        # enumerate(iterable, start) — даёт и номер (i) (начинаем с 1), и сам объект (uni)\n",
    "        for i, uni in enumerate(universities, 1):\n",
    "            log.info(f\"[{i}/{len(universities)}] {uni.get('name','')}\") # Пример: [1/50] Обработка: Oxford University\n",
    "\n",
    "            try:\n",
    "                # теперь вызываем parse_university_page, откуда для каждого уника уже достаются более подробные данные\n",
    "                detailed_data = self.parse_university_page(uni['url'])\n",
    "                full_data = {**uni, **detailed_data} # **uni - берём все пары ключ–значение из словаря uni, ан-но **detailed_data и всё объединяем в один новый словарь full_data (одинаковые ключи перезаписались автоматически как в uni)\n",
    "                all_data.append(full_data)\n",
    "\n",
    "                filled_fields = len([v for v in full_data.values() if v]) # просто посмотреть сколько получилось вытащить признаков по конркетному универу (сколько заполнено столбцов)\n",
    "                log.info(f\"  [OK] Собрано полей: {filled_fields}\")\n",
    "\n",
    "                # на всякий случай промежуточное сохранение\n",
    "                if i % save_every == 0:\n",
    "                    self.save_to_csv(all_data, f'backup_{output_file}')\n",
    "                    log.info(f\"  [SAVE] промежуточное сохранение: {i} записей\")\n",
    "\n",
    "            # если же произошла ошибка: (чтоб если что программа не упала, а перешла сюда)\n",
    "            except Exception as e:\n",
    "                log.exception(\"  [ERROR] ошибка при обработке университета\")\n",
    "                all_data.append(uni) # обрабатываем след уник\n",
    "\n",
    "        self.save_to_csv(all_data, output_file) # вызов ф-ии, чтоб сохранить\n",
    "        log.info(f\"[COMPLETE] данные сохранены в {output_file}\")\n",
    "        log.info(f\"[COMPLETE] всего университетов: {len(all_data)}\")\n",
    "\n",
    "        return all_data\n",
    "\n",
    "    # сохранение\n",
    "    def save_to_csv(self, data: List[Dict], filename: str):\n",
    "      if not data:\n",
    "        log.warning(\"[SAVE] нет данных для сохранения\")\n",
    "        return\n",
    "      df = pd.DataFrame(data)\n",
    "      df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "      log.info(f\"[SAVE] сохранено в {filename} (строк: {len(df)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oe6Sz26Los2q",
    "outputId": "8b3b0048-d426-4611-f369-5626dcbfe3a8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено карточек на странице 1: 10\n",
      "Всего собрано университетов: 10\n",
      "Найдено карточек на странице 2: 10\n",
      "Всего собрано университетов: 20\n",
      "Найдено карточек на странице 3: 10\n",
      "Всего собрано университетов: 30\n",
      "Найдено карточек на странице 4: 10\n",
      "Всего собрано университетов: 40\n",
      "Найдено карточек на странице 5: 10\n",
      "Всего собрано университетов: 50\n",
      "Найдено карточек на странице 6: 10\n",
      "Всего собрано университетов: 60\n",
      "Найдено карточек на странице 7: 10\n",
      "Всего собрано университетов: 70\n",
      "Найдено карточек на странице 8: 10\n",
      "Всего собрано университетов: 80\n",
      "Найдено карточек на странице 9: 10\n",
      "Всего собрано университетов: 90\n",
      "Найдено карточек на странице 10: 10\n",
      "Всего собрано университетов: 100\n"
     ]
    }
   ],
   "source": [
    "LIST_URL = \"https://www.unipage.net/en/universities\"\n",
    "\n",
    "scraper = UnipageScraper(delay=1.5)\n",
    "\n",
    "data = scraper.scrape_all(\n",
    "    list_url=LIST_URL,\n",
    "    output_file='universities_data.csv',\n",
    "    max_universities=10000,\n",
    "    save_every=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
