{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Scrapping (университеты)"
      ],
      "metadata": {
        "id": "yBc1Gn-Ut7qG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOoNBUiLjwYq",
        "outputId": "18bfb7c1-80f9-4b24-9255-c059ab02f535"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (5.4.0)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.12/dist-packages (4.38.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: trio<1.0,>=0.31.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (0.32.0)\n",
            "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in /usr/local/lib/python3.12/dist-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (1.9.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.31.0->selenium) (25.4.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.31.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.31.0->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.12/dist-packages (from trio-websocket<1.0,>=0.12.2->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.16.0)\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:2 https://cli.github.com/packages stable InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,856 kB]\n",
            "Fetched 4,244 kB in 3s (1,356 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "chromium-browser is already the newest version (1:85.0.4183.83-0ubuntu2.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 42 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4 lxml selenium\n",
        "!apt-get update\n",
        "!apt-get install -y chromium-browser"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time # для задержек между запросами (чтобы не блокировал сайт)\n",
        "from typing import Dict, List, Optional\n",
        "import re\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "eCysGIvBlGG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnipageScraper:\n",
        "\n",
        "    def __init__(self, delay: float = 1.5):\n",
        "        self.base_url = \"https://www.unipage.net\"\n",
        "        self.delay = delay\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36' # чтоб не заблокали\n",
        "        })\n",
        "\n",
        "    # загрузка страницы (по умолчанию request, если сработала ошибка (мало информации или в дальнейшем не нашел карточку университета), то пробуем вытащить данные, используя библиотеку selenium)\n",
        "    def get_page(self, url: str, retries: int = 3) -> Optional[BeautifulSoup]: # хотим получить html-код страницы (пробуем 3 раза)\n",
        "        for attempt in range(retries):\n",
        "            try:\n",
        "                 # вначале через request\n",
        "                response = self.session.get(url, timeout=30)\n",
        "                response.raise_for_status() # если сайт выдает ошибку (например, 404), то сразу переходим к блоку - except\n",
        "\n",
        "                html = response.text # сохраняем html код страницы\n",
        "                if len(html) < 500 or \"University\" not in html: # если страница пустая или слишком мало инфы- пробуем через Selenium\n",
        "                    raise ValueError(\"Пробуем через Selenium\") # вызываем ошибку в таком случае\n",
        "\n",
        "                time.sleep(self.delay)\n",
        "                return BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "            # если что-то не так: (сообщает об ошибке, но продолжает работать)\n",
        "            except Exception as e:\n",
        "                print(f\"[WARN] Ошибка при загрузке {url}: {e}\")\n",
        "\n",
        "                if attempt == retries - 1: # если это была последняя попытка - переходим к selenium\n",
        "\n",
        "                    # настройка драйвера\n",
        "                    opt = Options()\n",
        "                    opt.add_argument(\"--headless=new\") # без окна\n",
        "                    opt.add_argument(\"--no-sandbox\")\n",
        "                    opt.add_argument(\"--disable-dev-shm-usage\")\n",
        "                    opt.add_argument(\"--disable-gpu\")\n",
        "                    opt.add_argument(\"--window-size=1920,1080\")\n",
        "\n",
        "                    # запуск драйвера\n",
        "                    driver = webdriver.Chrome(options=opt)\n",
        "                    try:\n",
        "                        driver.get(url)\n",
        "                        time.sleep(2.5)\n",
        "                        html = driver.page_source\n",
        "                        return BeautifulSoup(html, \"html.parser\")\n",
        "                    finally:\n",
        "                        driver.quit()\n",
        "\n",
        "                time.sleep(5)\n",
        "        return None # если все попытки не удались - None\n",
        "\n",
        "    # доп ф-ия (извлечь число из текста, например, для рейтинга)\n",
        "    def extract_number(self, text: str) -> str:\n",
        "        match = re.search(r'[\\d,]+', text) # ищем посл-сть любых цифры 0-9 (\\d) и запятые\n",
        "        return match.group(0) if match else \"\"\n",
        "\n",
        "    # парсинг универа на главной странице (name, country, location (city), ranking_preview: QS, THE и ARWU, стоимость (degree type(bachelor, master и doctor)))\n",
        "    def parse_main_page_card(self, card) -> Dict:\n",
        "        data = {}\n",
        "\n",
        "        # название университета и ссылка\n",
        "        title_elem = card.select_one('.generated-card__title a') # ищем первый тег (select_one()) <a>, который находится внутри элемента с классом .generated-card__title\n",
        "        if title_elem:\n",
        "            data['name'] = title_elem.get_text(strip=True) # достаем название между тегами (без отступов)\n",
        "            data['url'] = self.base_url + title_elem.get('href', '') # достаём ссылку из атрибута тега <href> и соединяем\n",
        "        else:\n",
        "            data['name'] = \"\"\n",
        "            data['url'] = \"\"\n",
        "\n",
        "        # cтрана (по флагу)\n",
        "        flag_elem = card.select_one('.flag') # ищем все элем с классом .flag\n",
        "        if flag_elem:\n",
        "            flag_class = [c for c in flag_elem.get('class', []) if c.startswith('flag-')] # извлекаем список классов этого элем и отбираем только те, которе начинаются с 'flag-'\n",
        "            data['country'] = flag_class[0].replace('flag-', '').upper() if flag_class else \"\" # теперь для каждого в списке удаляем префикс 'flag-', получаем страну и записываем в вер[нем регистре\n",
        "        else:\n",
        "            data['country'] = \"\"\n",
        "\n",
        "        # город или локация\n",
        "        location_spans = card.select('.generated-card__row span') # ищем все элем с классом .generated-card__row span\n",
        "        if len(location_spans) >= 2:\n",
        "            data['location'] = location_spans[1].get_text(strip=True) # на странице есть несколько <span> и второй из них содержит город, поэтому мы берём location_spans[1]\n",
        "        else:\n",
        "            data['location'] = \"\"\n",
        "\n",
        "        # рейтинги QS, THE, ARWU\n",
        "        data['qs_ranking_preview'] = \"\"\n",
        "        data['the_ranking_preview'] = \"\"\n",
        "        data['arwu_ranking_preview'] = \"\"\n",
        "\n",
        "        tags = card.select('.tag_secondary') # ищем все элем с классом .tag_secondary\n",
        "        for tag in tags:\n",
        "            text = tag.get_text(strip=True)\n",
        "            # исп ф-ию, которая описана выше, чтоб вытащить цифры\n",
        "            if 'QS' in text:\n",
        "                data['qs_ranking_preview'] = self.extract_number(text)\n",
        "            elif 'THE' in text:\n",
        "                data['the_ranking_preview'] = self.extract_number(text)\n",
        "            elif 'ARWU' in text:\n",
        "                data['arwu_ranking_preview'] = self.extract_number(text)\n",
        "\n",
        "        # cтоимость обучения\n",
        "        data['bachelor_from'] = \"\"\n",
        "        data['bachelor_to'] = \"\"\n",
        "        data['master_from'] = \"\"\n",
        "        data['master_to'] = \"\"\n",
        "        data['doctorate_from'] = \"\"\n",
        "        data['doctorate_to'] = \"\"\n",
        "\n",
        "        tuition_cards = card.select('.content-card_inverted') # элем с классом .content-card_inverted, где указана стоимость\n",
        "        for tuition_card in tuition_cards: # перебираем все карточки\n",
        "            dt = tuition_card.select_one('dt') # ищем элемент <dt> - в нём написано для какой степени (бакалавр, магистр и тд)\n",
        "            if not dt:\n",
        "                continue\n",
        "\n",
        "            degree_type = dt.get_text(strip=True) # извлекаем из <dt> текст (удаляя лишние пробелы)\n",
        "            dds = tuition_card.select('dd .content-card-number__number') # ищем все элем с классом .content-card-number__number (cписок) внутри тега <dd>\n",
        "\n",
        "            if degree_type == 'Bachelor' and len(dds) >= 2:\n",
        "                data['bachelor_from'] = dds[0].get_text(strip=True)\n",
        "                data['bachelor_to'] = dds[1].get_text(strip=True)\n",
        "            elif degree_type == 'Master' and len(dds) >= 2:\n",
        "                data['master_from'] = dds[0].get_text(strip=True)\n",
        "                data['master_to'] = dds[1].get_text(strip=True)\n",
        "            elif degree_type == 'Doctorate' and len(dds) >= 2:\n",
        "                data['doctorate_from'] = dds[0].get_text(strip=True)\n",
        "                data['doctorate_to'] = dds[1].get_text(strip=True)\n",
        "\n",
        "        return data\n",
        "\n",
        "    # парсинг страницы каждого универа (location full, establishment year, students (кол-во), international students, rating - QS, THE и USA, female students, international students, acceptance rate)\n",
        "    def parse_university_page(self, url: str) -> Dict:\n",
        "        soup = self.get_page(url) # возвращаем объект BeautifulSoup (HTML-страница)\n",
        "        if not soup:\n",
        "            return {}\n",
        "\n",
        "        data = {}\n",
        "\n",
        "        infographic_cards = soup.select('.infographic-card__main') # вся нужная нам инфа в элементах с классом .infographic-card__main\n",
        "        for card in infographic_cards: # перебор\n",
        "            dt = card.select_one('dt.infographic-card__secondary-text') # название показателя\n",
        "            dd = card.select_one('dd.infographic-card__primary-text') # его значение\n",
        "\n",
        "            if dt and dd: # еслм оба есть, то берем текст без лишних пробелов\n",
        "                key = dt.get_text(strip=True)\n",
        "                value = dd.get_text(strip=True)\n",
        "\n",
        "                if key == 'Location':\n",
        "                    data['location_full'] = value\n",
        "                elif key == 'Establishment year':\n",
        "                    data['establishment_year'] = value\n",
        "                elif key == 'Students':\n",
        "                    data['total_students'] = value\n",
        "                elif key == 'International students':\n",
        "                    data['international_students'] = value\n",
        "                elif key == 'Female students':\n",
        "                    data['female_students'] = value\n",
        "                elif key == 'Acceptance rate':\n",
        "                    data['acceptance_rate'] = value\n",
        "\n",
        "        # теперь чекаем рейтинги\n",
        "        data['qs_rating'] = \"\"\n",
        "        data['the_rating'] = \"\"\n",
        "        data['rating_usa'] = \"\"\n",
        "\n",
        "        rating_buttons = soup.select('.chart-update-button') # вся нужная нам инфа в элементах с классом .chart-update-button\n",
        "        for button in rating_buttons: # перебор\n",
        "            title_elem = button.select_one('.content-card__title') # показатель\n",
        "            number_elem = button.select_one('.content-card-number__number') # значение\n",
        "\n",
        "            if title_elem and number_elem:\n",
        "                title = title_elem.get_text(strip=True)\n",
        "                number = number_elem.get_text(strip=True)\n",
        "\n",
        "                if 'Rating QS' in title:\n",
        "                    data['qs_rating'] = number\n",
        "                elif 'Rating THE' in title:\n",
        "                    data['the_rating'] = number\n",
        "                elif 'Rating in the USA' in title:\n",
        "                    data['rating_usa'] = number\n",
        "\n",
        "        return data\n",
        "\n",
        "    # cбор списков универов (перебираем страницы, пока не найдем нужное кол-во (10к+), и сохраняем в список ссылки карточек универов)\n",
        "    def scrape_universities_list(self, base_list_url: str, max_universities: int = 50) -> List[Dict]: # (по умолчанию 50, но мы потом говорим свое значение)\n",
        "        universities = []\n",
        "        page = 1\n",
        "\n",
        "        # перебор страниц, пока не соберем нужное кол-во универов\n",
        "        while len(universities) < max_universities:\n",
        "            if page == 1:\n",
        "                url = base_list_url\n",
        "            else:\n",
        "                url = f\"{base_list_url}?page={page}\"\n",
        "\n",
        "            print(f\"\\nЗагрузка страницы {page}: {url}\")\n",
        "            soup = self.get_page(url) # через get_page() берём HTML страницы\n",
        "\n",
        "            if not soup:\n",
        "                print(f\"Не удалось загрузить страницу {page}\")\n",
        "                break\n",
        "\n",
        "            cards = soup.select('.generated-card') # получаем список всех карточек на странице\n",
        "\n",
        "            if not cards:\n",
        "                print(f\"На странице {page} не найдено карточек. Завершение.\")\n",
        "                break\n",
        "\n",
        "            print(f\"Найдено карточек на странице {page}: {len(cards)}\")\n",
        "\n",
        "            for card in cards:\n",
        "                if len(universities) >= max_universities:\n",
        "                    break\n",
        "\n",
        "                # теперь для каждой карточки вызываем парсер:\n",
        "                uni_data = self.parse_main_page_card(card)\n",
        "                if uni_data.get('url'):\n",
        "                    universities.append(uni_data)\n",
        "\n",
        "            print(f\"Всего собрано университетов: {len(universities)}\")\n",
        "\n",
        "            if len(universities) >= max_universities:\n",
        "                break\n",
        "\n",
        "            page += 1\n",
        "\n",
        "        return universities\n",
        "\n",
        "    # полный сбор всех данных (главная ф-ия, она вызывает остальные в этом классе)\n",
        "    def scrape_all(self, list_url: str, output_file: str = 'universities.csv',\n",
        "                   max_universities: int = 50, save_every: int = 25):\n",
        "        print(\"Начинаем сбор ссылок на университеты...\")\n",
        "\n",
        "        # сначала вызываем scrape_universities_list - список словарей, где у каждого универа будут уже данные (которые с главной страницы)\n",
        "        universities = self.scrape_universities_list(list_url, max_universities)\n",
        "\n",
        "        print(f\"\\nБудет обработано университетов: {len(universities)}\")\n",
        "\n",
        "        all_data = []\n",
        "\n",
        "        # проходимся по универам\n",
        "        # enumerate(iterable, start) — даёт и номер (i) (начинаем с 1), и сам объект (uni)\n",
        "        for i, uni in enumerate(universities, 1):\n",
        "            print(f\"\\n[{i}/{len(universities)}] Обработка: {uni['name']}\") # Пример: [1/50] Обработка: Oxford University\n",
        "\n",
        "            try:\n",
        "                # теперь вызываем parse_university_page, откуда для каждого уника уже достаются более подробные данные\n",
        "                detailed_data = self.parse_university_page(uni['url'])\n",
        "                full_data = {**uni, **detailed_data} # **uni - берём все пары ключ–значение из словаря uni, ан-но **detailed_data и всё объединяем в один новый словарь full_data (одинаковые ключи перезаписались автоматически как в uni)\n",
        "                all_data.append(full_data)\n",
        "\n",
        "                filled_fields = len([v for v in full_data.values() if v]) # просто посмотреть сколько получилось вытащить признаков по конркетному универу (сколько заполнено столбцов)\n",
        "                print(f\"  [OK] Собрано полей: {filled_fields}\")\n",
        "\n",
        "                # на всякий случай промежуточное сохранение\n",
        "                if i % save_every == 0:\n",
        "                    self.save_to_csv(all_data, f'backup_{output_file}')\n",
        "                    print(f\"  [SAVE] Промежуточное сохранение: {i} записей\")\n",
        "\n",
        "            # если же произошла ошибка: (чтоб если что программа не упала, а перешла сюда)\n",
        "            except Exception as e:\n",
        "                print(f\"  [ERROR] Ошибка при обработке: {e}\")\n",
        "                all_data.append(uni) # обрабатываем след уник\n",
        "\n",
        "        self.save_to_csv(all_data, output_file) # вызов ф-ии, чтоб сохранить\n",
        "        print(f\"\\n[COMPLETE] Данные сохранены в {output_file}\")\n",
        "        print(f\"[COMPLETE] Всего университетов: {len(all_data)}\")\n",
        "\n",
        "        return all_data\n",
        "\n",
        "    # сохранение\n",
        "    def save_to_csv(self, data: List[Dict], filename: str):\n",
        "      if not data:\n",
        "        print(\"Нет данных для сохранения\")\n",
        "        return\n",
        "      df = pd.DataFrame(data)\n",
        "      df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
        "      print(f\"[SAVE] Сохранено в {filename}\")"
      ],
      "metadata": {
        "id": "ByICJGAslITm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LIST_URL = \"https://www.unipage.net/en/universities\"\n",
        "\n",
        "scraper = UnipageScraper(delay=1.5)\n",
        "\n",
        "data = scraper.scrape_all(\n",
        "    list_url=LIST_URL,\n",
        "    output_file='universities_data.csv',\n",
        "    max_universities=10000,\n",
        "    save_every=25\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oe6Sz26Los2q",
        "outputId": "8b3b0048-d426-4611-f369-5626dcbfe3a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Начинаем сбор ссылок на университеты...\n",
            "\n",
            "Загрузка страницы 1: https://www.unipage.net/en/universities\n",
            "Найдено карточек на странице 1: 10\n",
            "Всего собрано университетов: 10\n",
            "\n",
            "Будет обработано университетов: 10\n",
            "\n",
            "[1/10] Обработка: Harvard University\n",
            "  [OK] Собрано полей: 22\n",
            "\n",
            "[2/10] Обработка: Massachusetts Institute of Technology\n",
            "  [OK] Собрано полей: 22\n",
            "\n",
            "[3/10] Обработка: Stanford University\n",
            "  [OK] Собрано полей: 22\n",
            "\n",
            "[4/10] Обработка: University of Cambridge\n",
            "  [OK] Собрано полей: 21\n",
            "\n",
            "[5/10] Обработка: California Institute of Technology\n",
            "  [OK] Собрано полей: 22\n",
            "\n",
            "[6/10] Обработка: University of Oxford\n",
            "  [OK] Собрано полей: 21\n",
            "\n",
            "[7/10] Обработка: Princeton University\n",
            "  [OK] Собрано полей: 20\n",
            "\n",
            "[8/10] Обработка: University of Chicago\n",
            "  [OK] Собрано полей: 17\n",
            "\n",
            "[9/10] Обработка: University College London\n",
            "  [OK] Собрано полей: 18\n",
            "\n",
            "[10/10] Обработка: ETH Zürich\n",
            "  [OK] Собрано полей: 20\n",
            "[SAVE] Сохранено в universities_data.csv\n",
            "\n",
            "[COMPLETE] Данные сохранены в universities_data.csv\n",
            "[COMPLETE] Всего университетов: 10\n"
          ]
        }
      ]
    }
  ]
}